#!/usr/bin/env bash
set -e

mkdir --parents /etc/kubernetes/{manifests,bootstrap-configs,bootstrap-manifests}

if ! podman inspect {{.ReleaseImage}} &>/dev/null; then
    echo "Pulling release image..."
    podman pull --quiet {{.ReleaseImage}}
fi

# convert the release image pull spec to an "absolute" form if a digest is available - this is
# safe to resolve after the actions above because podman will not pull the image once it is
# locally available
if ! release=$( podman inspect {{.ReleaseImage}} -f '{{"{{"}} index .RepoDigests 0 {{"}}"}}' ) || [[ -z "${release}" ]]; then
	echo "Warning: Could not resolve release image to pull by digest" 2>&1
	release="{{.ReleaseImage}}"
fi

MACHINE_CONFIG_OPERATOR_IMAGE=$(podman run --quiet --rm ${release} image machine-config-operator)
MACHINE_CONFIG_CONTROLLER_IMAGE=$(podman run --quiet --rm ${release} image machine-config-controller)
MACHINE_CONFIG_SERVER_IMAGE=$(podman run --quiet --rm ${release} image machine-config-server)
MACHINE_CONFIG_DAEMON_IMAGE=$(podman run --quiet --rm ${release} image machine-config-daemon)
MACHINE_CONFIG_OSCONTENT=$(podman run --quiet --rm ${release} image machine-os-content)
MACHINE_CONFIG_ETCD_IMAGE=$(podman run --quiet --rm ${release} image etcd)
MACHINE_CONFIG_SETUP_ETCD_ENV_IMAGE=$(podman run --quiet --rm ${release} image setup-etcd-environment)
MACHINE_CONFIG_KUBE_CLIENT_AGENT_IMAGE=$(podman run --quiet --rm ${release} image kube-client-agent)
MACHINE_CONFIG_INFRA_IMAGE=$(podman run --quiet --rm ${release} image pod)

KUBE_ETCD_SIGNER_SERVER_IMAGE=$(podman run --quiet --rm ${release} image kube-etcd-signer-server)

CONFIG_OPERATOR_IMAGE=$(podman run --quiet --rm ${release} image cluster-config-operator)
KUBE_APISERVER_OPERATOR_IMAGE=$(podman run --quiet --rm ${release} image cluster-kube-apiserver-operator)
KUBE_CONTROLLER_MANAGER_OPERATOR_IMAGE=$(podman run --quiet --rm ${release} image cluster-kube-controller-manager-operator)
KUBE_SCHEDULER_OPERATOR_IMAGE=$(podman run --quiet --rm ${release} image cluster-kube-scheduler-operator)

OPENSHIFT_HYPERSHIFT_IMAGE=$(podman run --quiet --rm ${release} image hypershift)
OPENSHIFT_HYPERKUBE_IMAGE=$(podman run --quiet --rm ${release} image hyperkube)

CLUSTER_BOOTSTRAP_IMAGE=$(podman run --quiet --rm ${release} image cluster-bootstrap)

mkdir --parents ./{bootstrap-manifests,manifests}

if [ ! -f cvo-bootstrap.done ]
then
	echo "Rendering Cluster Version Operator Manifests..."

	rm -rf cvo-bootstrap

	# shellcheck disable=SC2154
	podman run \
		--quiet \
		--volume "$PWD:/assets:z" \
		"${release}" \
		render \
			--output-dir=/assets/cvo-bootstrap \
			--release-image="${release}"

	cp cvo-bootstrap/bootstrap/* bootstrap-manifests/
	cp cvo-bootstrap/manifests/* manifests/
	## FIXME: CVO should use `/etc/kubernetes/bootstrap-secrets/kubeconfig` instead
	cp auth/kubeconfig /etc/kubernetes/kubeconfig

	touch cvo-bootstrap.done
fi

if [ ! -f config-bootstrap.done ]
then
	echo "Rendering cluster config manifests..."

	rm -rf config-bootstrap

	# shellcheck disable=SC2154
	podman run \
		--quiet \
		--volume "$PWD:/assets:z" \
		"${CONFIG_OPERATOR_IMAGE}" \
		/usr/bin/cluster-config-operator render \
		--config-output-file=/assets/config-bootstrap/config \
		--asset-input-dir=/assets/tls \
		--asset-output-dir=/assets/config-bootstrap

	cp config-bootstrap/manifests/* manifests/

	touch config-bootstrap.done
fi

if [ ! -f kube-apiserver-bootstrap.done ]
then
	echo "Rendering Kubernetes API server core manifests..."

	rm -rf kube-apiserver-bootstrap

	# shellcheck disable=SC2154
	podman run \
		--quiet \
		--volume "$PWD:/assets:z" \
		"${KUBE_APISERVER_OPERATOR_IMAGE}" \
		/usr/bin/cluster-kube-apiserver-operator render \
		--manifest-etcd-serving-ca=etcd-ca-bundle.crt \
		--manifest-etcd-server-urls={{.EtcdCluster}} \
		--manifest-image=${OPENSHIFT_HYPERSHIFT_IMAGE} \
		--asset-input-dir=/assets/tls \
		--asset-output-dir=/assets/kube-apiserver-bootstrap \
		--config-output-file=/assets/kube-apiserver-bootstrap/config \
		--cluster-config-file=/assets/manifests/cluster-network-02-config.yml

	cp kube-apiserver-bootstrap/config /etc/kubernetes/bootstrap-configs/kube-apiserver-config.yaml
	cp kube-apiserver-bootstrap/bootstrap-manifests/* bootstrap-manifests/
	cp kube-apiserver-bootstrap/manifests/* manifests/

	touch kube-apiserver-bootstrap.done
fi

if [ ! -f kube-controller-manager-bootstrap.done ]
then
	echo "Rendering Kubernetes Controller Manager core manifests..."

	rm -rf kube-controller-manager-bootstrap

	# shellcheck disable=SC2154
	podman run \
		--quiet \
		--volume "$PWD:/assets:z" \
		"${KUBE_CONTROLLER_MANAGER_OPERATOR_IMAGE}" \
		/usr/bin/cluster-kube-controller-manager-operator render \
		--manifest-image=${OPENSHIFT_HYPERKUBE_IMAGE} \
		--asset-input-dir=/assets/tls \
		--asset-output-dir=/assets/kube-controller-manager-bootstrap \
		--config-output-file=/assets/kube-controller-manager-bootstrap/config \
		--cluster-config-file=/assets/manifests/cluster-network-02-config.yml

	cp kube-controller-manager-bootstrap/config /etc/kubernetes/bootstrap-configs/kube-controller-manager-config.yaml
	cp kube-controller-manager-bootstrap/bootstrap-manifests/* bootstrap-manifests/
	cp kube-controller-manager-bootstrap/manifests/* manifests/

	touch kube-controller-manager-bootstrap.done
fi

if [ ! -f kube-scheduler-bootstrap.done ]
then
	echo "Rendering Kubernetes Scheduler core manifests..."

	rm -rf kube-scheduler-bootstrap

	# shellcheck disable=SC2154
	podman run \
		--quiet \
		--volume "$PWD:/assets:z" \
		"${KUBE_SCHEDULER_OPERATOR_IMAGE}" \
		/usr/bin/cluster-kube-scheduler-operator render \
		--manifest-image=${OPENSHIFT_HYPERKUBE_IMAGE} \
		--asset-input-dir=/assets/tls \
		--asset-output-dir=/assets/kube-scheduler-bootstrap \
		--config-output-file=/assets/kube-scheduler-bootstrap/config

	cp kube-scheduler-bootstrap/config /etc/kubernetes/bootstrap-configs/kube-scheduler-config.yaml
	cp kube-scheduler-bootstrap/bootstrap-manifests/* bootstrap-manifests/
	cp kube-scheduler-bootstrap/manifests/* manifests/

	touch kube-scheduler-bootstrap.done
fi


# We originally wanted to run the etcd cert signer as
# a static pod, but kubelet could't remove static pod
# when API server is not up, so we have to run this as
# podman container.
# See https://github.com/kubernetes/kubernetes/issues/43292

echo "Starting etcd certificate signer..."

trap "podman rm --force etcd-signer" ERR

# shellcheck disable=SC2154
podman run \
	--quiet \
	--name etcd-signer \
	--detach \
	--volume /opt/openshift/tls:/opt/openshift/tls:ro,z \
	--network host \
	"${KUBE_ETCD_SIGNER_SERVER_IMAGE}" \
	serve \
	--cacrt=/opt/openshift/tls/etcd-signer.crt \
	--cakey=/opt/openshift/tls/etcd-signer.key \
	--metric-cacrt=/opt/openshift/tls/etcd-metric-signer.crt \
	--metric-cakey=/opt/openshift/tls/etcd-metric-signer.key \
	--servcrt=/opt/openshift/tls/kube-apiserver-lb-server.crt \
	--servkey=/opt/openshift/tls/kube-apiserver-lb-server.key \
	--servcrt=/opt/openshift/tls/kube-apiserver-internal-lb-server.crt \
	--servkey=/opt/openshift/tls/kube-apiserver-internal-lb-server.key \
	--address=0.0.0.0:6443 \
	--csrdir=/tmp \
	--peercertdur=26280h \
	--servercertdur=26280h \
	--metriccertdur=26280h


# Wait for CA bundle generation... (maybe unnecessary?)
# TODO poll for real
sleep 5

echo "Copying etcd CA certs"
mkdir -p /etc/kubernetes/static-pod-resources/etcd-member
cp -v /opt/openshift/tls/etcd-ca-bundle.crt /etc/kubernetes/static-pod-resources/etcd-member/ca.crt
cp -v /opt/openshift/tls/etcd-metric-ca-bundle.crt /etc/kubernetes/static-pod-resources/etcd-member/metrics-ca.crt

# TODO figure out where this comes from on the masters and reuse it
echo "Writing etcd config"
mkdir -p /etc/etcd
cat <<'EOF' > /etc/etcd/etcd.conf
#[member]
ETCD_SNAPSHOT_COUNT=100000
ETCD_HEARTBEAT_INTERVAL=100
ETCD_ELECTION_TIMEOUT=1000

#[storage]
ETCD_QUOTA_BACKEND_BYTES=7516192768

#[logging]
ETCD_DEBUG=false

#[profiling]
ETCD_ENABLE_PPROF=false
EOF

# TODO figure out where this comes from on the masters and reuse it
echo "Writing bootstrap etcd manifest..."
cat <<'EOF' > /etc/kubernetes/manifests/etcd-member.yaml
apiVersion: v1
kind: Pod
metadata:
  name: etcd-member
  namespace: openshift-etcd
  labels:
    k8s-app: etcd
spec:
  initContainers:
  - name: discovery
    image: "registry.svc.ci.openshift.org/origin/4.1-2019-05-13-204940@sha256:79453fd6f13c7476d02cd9445bd7e5d0a3caaeb007b02f2869cb9dcbc1bb5398"
    args:
    - "run"
    - "--discovery-srv=jeckersb.devcluster.openshift.com"
    - "--output-file=/run/etcd/environment"
    - "--v=4"
    volumeMounts:
    - name: discovery
      mountPath: /run/etcd/
  - name: certs
    image: "registry.svc.ci.openshift.org/origin/4.1-2019-05-13-204940@sha256:ba7d3e54ceca7484342c15c78b11f3373dbec3f4cf82bfd32d3d165b68b56d0a"
    command:
    - /bin/sh
    - -c
    - |
      #!/bin/sh
      set -euxo pipefail

      source /run/etcd/environment

      [ -e /etc/ssl/etcd/system:etcd-server:${ETCD_DNS_NAME}.crt -a \
        -e /etc/ssl/etcd/system:etcd-server:${ETCD_DNS_NAME}.key ] || \
        kube-client-agent \
          request \
            --kubeconfig=/etc/kubernetes/kubeconfig \
            --orgname=system:etcd-servers \
            --assetsdir=/etc/ssl/etcd \
            --dnsnames=localhost,etcd.kube-system.svc,etcd.kube-system.svc.cluster.local,etcd.openshift-etcd.svc,etcd.openshift-etcd.svc.cluster.local,${ETCD_WILDCARD_DNS_NAME} \
            --commonname=system:etcd-server:${ETCD_DNS_NAME} \
            --ipaddrs=${ETCD_IPV4_ADDRESS},127.0.0.1 \

      [ -e /etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.crt -a \
        -e /etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.key ] || \
        kube-client-agent \
          request \
            --kubeconfig=/etc/kubernetes/kubeconfig \
            --orgname=system:etcd-peers \
            --assetsdir=/etc/ssl/etcd \
            --dnsnames=${ETCD_DNS_NAME},jeckersb.devcluster.openshift.com \
            --commonname=system:etcd-peer:${ETCD_DNS_NAME} \
            --ipaddrs=${ETCD_IPV4_ADDRESS} \

      [ -e /etc/ssl/etcd/system:etcd-metric:${ETCD_DNS_NAME}.crt -a \
        -e /etc/ssl/etcd/system:etcd-metric:${ETCD_DNS_NAME}.key ] || \
        kube-client-agent \
          request \
            --kubeconfig=/etc/kubernetes/kubeconfig \
            --orgname=system:etcd-metrics \
            --assetsdir=/etc/ssl/etcd \
            --dnsnames=localhost,etcd.kube-system.svc,etcd.kube-system.svc.cluster.local,etcd.openshift-etcd.svc,etcd.openshift-etcd.svc.cluster.local,${ETCD_WILDCARD_DNS_NAME} \
            --commonname=system:etcd-metric:${ETCD_DNS_NAME} \
            --ipaddrs=${ETCD_IPV4_ADDRESS} \

    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - name: discovery
      mountPath: /run/etcd/
    - name: certs
      mountPath: /etc/ssl/etcd/
    - name: kubeconfig
      mountPath: /etc/kubernetes/kubeconfig
  containers:
  - name: etcd-member
    image: "registry.svc.ci.openshift.org/origin/4.1-2019-05-13-204940@sha256:17b4446c44e90a7b50cf2d941127a1c78dd36f191cd728b1527ea9fd676c6b17"
    command:
    - /bin/sh
    - -c
    - |
      #!/bin/sh
      set -euo pipefail

      source /run/etcd/environment

      set -a
      source /etc/etcd/etcd.conf
      set +a

      exec etcd \
        --initial-advertise-peer-urls=https://${ETCD_IPV4_ADDRESS}:2380 \
        --cert-file=/etc/ssl/etcd/system:etcd-server:${ETCD_DNS_NAME}.crt \
        --key-file=/etc/ssl/etcd/system:etcd-server:${ETCD_DNS_NAME}.key \
        --trusted-ca-file=/etc/ssl/etcd/ca.crt \
        --client-cert-auth=true \
        --peer-cert-file=/etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.crt \
        --peer-key-file=/etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.key \
        --peer-trusted-ca-file=/etc/ssl/etcd/ca.crt \
        --peer-client-cert-auth=true \
        --advertise-client-urls=https://${ETCD_IPV4_ADDRESS}:2379 \
        --listen-client-urls=https://0.0.0.0:2379 \
        --listen-peer-urls=https://0.0.0.0:2380 \
        --listen-metrics-urls=https://0.0.0.0:9978 \
    resources:
      requests:
        memory: 600Mi
        cpu: 300m
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - name: discovery
      mountPath: /run/etcd/
    - name: certs
      mountPath: /etc/ssl/etcd/
    - name: data-dir
      mountPath: /var/lib/etcd/
    - name: conf
      mountPath: /etc/etcd/

    env:
    - name: ETCD_DATA_DIR
      value: "/var/lib/etcd"
    - name: ETCD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    ports:
    - name: peer
      containerPort: 2380
      protocol: TCP
    - name: server
      containerPort: 2379
      protocol: TCP
  - name: etcd-metrics
    image: "registry.svc.ci.openshift.org/origin/4.1-2019-05-13-204940@sha256:17b4446c44e90a7b50cf2d941127a1c78dd36f191cd728b1527ea9fd676c6b17"
    command:
    - /bin/sh
    - -c
    - |
      #!/bin/sh
      set -euo pipefail

      source /run/etcd/environment

      exec etcd grpc-proxy start \
        --endpoints https://${ETCD_DNS_NAME}:9978 \
        --metrics-addr https://0.0.0.0:9979 \
        --listen-addr 127.0.0.1:9977 \
        --key /etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.key \
        --key-file /etc/ssl/etcd/system:etcd-metric:${ETCD_DNS_NAME}.key \
        --cert /etc/ssl/etcd/system:etcd-peer:${ETCD_DNS_NAME}.crt \
        --cert-file /etc/ssl/etcd/system:etcd-metric:${ETCD_DNS_NAME}.crt \
        --cacert /etc/ssl/etcd/ca.crt \
        --trusted-ca-file /etc/ssl/etcd/metric-ca.crt \
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - name: discovery
      mountPath: /run/etcd/
    - name: certs
      mountPath: /etc/ssl/etcd/
    ports:
    - name: metric
      containerPort: 9979
      protocol: TCP
  hostNetwork: true
  priorityClassName: system-node-critical
  tolerations:
  - operator: "Exists"
  restartPolicy: Always
  volumes:
  - name: certs
    hostPath:
      path: /etc/kubernetes/static-pod-resources/etcd-member
  - name: kubeconfig
    hostPath:
      path: /etc/kubernetes/kubeconfig
  - name: discovery
    hostPath:
      path: /run/etcd
  - name: data-dir
    hostPath:
      path: /var/lib/etcd
  - name: conf
    hostPath:
      path: /etc/etcd
EOF


if [ ! -f mco-bootstrap.done ]
then
	echo "Rendering MCO manifests..."

	rm -rf mco-bootstrap

	# shellcheck disable=SC2154
	podman run \
		--quiet \
		--user 0 \
		--volume "$PWD:/assets:z" \
		"${MACHINE_CONFIG_OPERATOR_IMAGE}" \
		bootstrap \
			--etcd-ca=/assets/tls/etcd-ca-bundle.crt \
			--etcd-metric-ca=/assets/tls/etcd-metric-ca-bundle.crt \
			--root-ca=/assets/tls/root-ca.crt \
			--kube-ca=/assets/tls/kube-apiserver-complete-client-ca-bundle.crt \
			--config-file=/assets/manifests/cluster-config.yaml \
			--dest-dir=/assets/mco-bootstrap \
			--pull-secret=/assets/manifests/openshift-config-secret-pull-secret.yaml \
			--etcd-image=${MACHINE_CONFIG_ETCD_IMAGE} \
			--setup-etcd-env-image=${MACHINE_CONFIG_SETUP_ETCD_ENV_IMAGE} \
			--kube-client-agent-image=${MACHINE_CONFIG_KUBE_CLIENT_AGENT_IMAGE} \
			--machine-config-controller-image=${MACHINE_CONFIG_CONTROLLER_IMAGE} \
			--machine-config-server-image=${MACHINE_CONFIG_SERVER_IMAGE} \
			--machine-config-daemon-image=${MACHINE_CONFIG_DAEMON_IMAGE} \
			--machine-config-oscontent-image=${MACHINE_CONFIG_OSCONTENT} \
			--infra-image=${MACHINE_CONFIG_INFRA_IMAGE} \
			--cloud-config-file=/assets/manifests/cloud-provider-config.yaml

	# Bootstrap MachineConfigController uses /etc/mcc/bootstrap/manifests/ dir to
	# 1. read the controller config rendered by MachineConfigOperator
	# 2. read the default MachineConfigPools rendered by MachineConfigOperator
	# 3. read any additional MachineConfigs that are needed for the default MachineConfigPools.
	mkdir --parents /etc/mcc/bootstrap /etc/mcs/bootstrap /etc/kubernetes/manifests
	cp mco-bootstrap/bootstrap/manifests/* /etc/mcc/bootstrap/
	cp openshift/* /etc/mcc/bootstrap/
	cp auth/kubeconfig-kubelet /etc/mcs/kubeconfig
	cp mco-bootstrap/bootstrap/machineconfigoperator-bootstrap-pod.yaml /etc/kubernetes/manifests/
	cp mco-bootstrap/manifests/* manifests/

	# /etc/ssl/mcs/tls.{crt, key} are locations for MachineConfigServer's tls assets.
	mkdir --parents /etc/ssl/mcs/
	cp tls/machine-config-server.crt /etc/ssl/mcs/tls.crt
	cp tls/machine-config-server.key /etc/ssl/mcs/tls.key

	touch mco-bootstrap.done
fi


echo "Waiting for etcd cluster..."

# Wait for the etcd cluster to come up.
# shellcheck disable=SC2154,SC2086
until podman run \
		--quiet \
		--rm \
		--network host \
		--name etcdctl \
		--env ETCDCTL_API=3 \
		--volume /opt/openshift/tls:/opt/openshift/tls:ro,z \
		--entrypoint etcdctl \
		"${MACHINE_CONFIG_ETCD_IMAGE}" \
		--dial-timeout=10m \
		--cacert=/opt/openshift/tls/etcd-ca-bundle.crt \
		--cert=/opt/openshift/tls/etcd-client.crt \
		--key=/opt/openshift/tls/etcd-client.key \
		--endpoints={{.EtcdCluster}} \
		endpoint health
do
	echo "etcdctl failed. Retrying in 5 seconds..."
	sleep 5
done

echo "etcd cluster up. Killing etcd certificate signer..."

podman rm --force etcd-signer
rm --force /etc/kubernetes/manifests/machineconfigoperator-bootstrap-pod.yaml

echo "Starting cluster-bootstrap..."

# shellcheck disable=SC2154
podman run \
	--quiet \
	--rm \
	--volume "$PWD:/assets:z" \
	--volume /etc/kubernetes:/etc/kubernetes:z \
	--network=host \
	"${CLUSTER_BOOTSTRAP_IMAGE}" \
	start --asset-dir=/assets --required-pods="openshift-kube-apiserver/kube-apiserver,openshift-kube-scheduler/openshift-kube-scheduler,openshift-kube-controller-manager/kube-controller-manager,openshift-cluster-version/cluster-version-operator"

# Workaround for https://github.com/opencontainers/runc/pull/1807
touch /opt/openshift/.bootkube.done
echo "bootkube.service complete"
